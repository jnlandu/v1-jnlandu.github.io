<div class="row ">
    <h1 class="h3 ">Courses attended</p>
    <h1 class="h6 text-white "> In this section , you can find some of the courses
         that attended.
    </h1>
    <div class="">
    <h1 class="h5 yellow"> Foundation of Machine Learning</h1>
    <div class="h6 text-white">
        This course was provided by Prof. <a class="yellow" href="https://scholar.google.com/citations?user=5wuvTfoAAAAJ&hl=en">Moustapha Cisse</a>. The course had two parts: Statistical learning Theory  and an introductoion to Deep Learning. In the first, we delved into theoretical aspects of machine learning, namely the statistical learning theory,
         where we discussed of supervised and unsupervised learning. We discussed of the ERM problem, Variance-Bias tradeoff, Overfitting and Underfitting, Linear Regression, Classification, Generalized linear models, and dimensionality reduction techiques such:
         <ul class="h6 text-white">
            <li>Random projections</li>
            <li> Principal Component Anlysis</li>
            <li> Prinicpal Canonical Analysis</li>
         </ul> 
        In the second part, we delved into Neural network, weights initialization, CNN, RCNN, Attention Mechanism and Transformers.<br>
        Please, find <a class="yellow" href="here">here</a> the materials I used and some codes that I implemented. Also check my tutorials below.
    </p>
    <h1 class="h5 yellow">Kernel Methods for Machine Learning </h1>
    This courses was given by  <a class="yellow" href="https://jpvert.github.io/">J.P Vert</a> from  <a class="yellow" href="https://www.owkin.com/">Owkin</a> and the T. A  Menegeaux and Juliette-Marie from Inria.
    This course  covered basic concepts of machine learning in high dimension, and the importance of regularization. We also discussed about Support Vector Machine (SVM).
    We studied in detail high-dimensional libnear models regularized by euclidean norm or the manhattan norm, including the ridge and lasso
    regression, the ridge classification. One of the key ideas of this course
    was to show us that, some learning problems which can be intricated to solve in finite dimensional euclidean space, can easily be solved
    in  infinite dimensional spaces, provided a good feature mapping.  The kernel is just a similarity function between data points. We them showed how positive kernel, via the representer theorem, transform the ERM problem of euclidean space into  an ERM problem of large dimensional space. Kernels allows to transform linear models into non-linear models
    , usable even for non-vectorial data such as strings and graphs. We also discussed 
    Reproducing Kernel Hilbert Spaces (RKHS), and related theorems. Please find my <a class="yellow" href="codes">codes</a> and the course page <a class="yellow" href="here">here</a>.
</div>